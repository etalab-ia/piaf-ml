{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, on fait une analyse quantitative pour déterminer à quel niveau les deux retrievers sont complémentaries. On essaie ensuite de déterminer des seuils pour le score qui nous permetteraient de choisir un des deux/en faire un mélange pour incrementer la performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_json = json.load(open(\"f615_detailed_results.json\"))\n",
    "sparse_json = json.load(open(\"9abf_detailed_results.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dense_errors = dense_json[\"errors\"]\n",
    "sparse_errors = sparse_json[\"errors\"]\n",
    "dense_successes = dense_json[\"successes\"]\n",
    "sparse_successes = sparse_json[\"successes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_bon = set(sparse_successes.keys())\n",
    "sparse_mauvais = set(sparse_errors.keys())\n",
    "dense_bon = set(dense_successes.keys())\n",
    "dense_mauvais = set(dense_errors.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erreurs communes au deux retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etant donné notre échantillon de réponses correctes et d'erreurs pour le retriever sparse et le retriever dense, vérifions tout d'abord si les deux se trompent souvent sur les memes questions et si oui, en quel pourcentage par rapport aux errors commis au total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_errors = list(dense_mauvais.intersection(sparse_mauvais))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "0.2664359861591695\n"
     ]
    }
   ],
   "source": [
    "print(len(common_errors))\n",
    "print(len(common_errors)/(len(dense_errors)+len(sparse_errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dans cet échantillon, 26% des erreurs commis sont commis par les deux retrivers. Il est peut etre intéressant d'aller plus loin dans l'analyse de ces erreurs qui semblent très difficiles à résoudre pour les deux retrievers.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation des cas où le retriever dense fait des erreurs et le sparse trouve la bonne réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "dense_fails_sparse_good = list(sparse_bon.intersection(dense_mauvais))\n",
    "print(len(dense_fails_sparse_good))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pour 95 questions, le retriever sparse donne des bonnes réponses là où le dense se trompe.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5523255813953488\n"
     ]
    }
   ],
   "source": [
    "print(len(dense_fails_sparse_good)/len(dense_mauvais))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sur les 172 erreurs faites par le retriever dense, dans 55% des cas le sparse peut donner la bonne réponse, et il pourrait donc etre utile de mélanger les deux.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation des cas où le retriever sparse fait des erreurs et le dense trouve la bonne réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "sparse_fails_dense_good = list(sparse_mauvais.intersection(dense_bon))\n",
    "print(len(sparse_fails_dense_good))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pour 40 questions, le retriever dense donne des bonnes réponses là où le sparse se trompe.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3418803418803419\n"
     ]
    }
   ],
   "source": [
    "print(len(sparse_fails_dense_good)/len(sparse_mauvais))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sur les 117 erreurs faites par le retriever sparse, dans 34% des cas le dense peut donner la bonne réponse.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "Le retriever sparse, si mélangé au retriever dense, peut aider à l'améliorer (le sparse donne globalement des meilleures réponses donc c'est assez normal). Dans 34% des cas, le retriever dense peut apporter des bonnes réponses là où le sparse se trompe. Globalement, on ne peut pas parler de complémentarité exacte (dans 26% des cas les deux se trompent et un mélange des deux n'apporteraient donc aucune amélioration), mais coupler le sparse au dense peut augmenter le nombre de bonnes réponses par rapport au dense seul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suite de l'analyse: peut-on trouver un seuil pour 'score' ou 'proba' qui nous indique qu'il faudrait utiliser le retriever sparse pour améliorer le dense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*exemple de ce qu'on souhaiterait avoir : if [score (dense) < 0.2 et score (sparse) > 0.7 ]: use sparse*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un dataframe qui contient la liste des questions où le retriever sparse améliore le dense, avec les scores et les probas correspondantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>score moyen_dense</th>\n",
       "      <th>score moyen_sparse</th>\n",
       "      <th>proba moyenne_dense</th>\n",
       "      <th>proba moyen_sparse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quellel est la date limite pour demander une b...</td>\n",
       "      <td>0.480306</td>\n",
       "      <td>19.480194</td>\n",
       "      <td>0.740153</td>\n",
       "      <td>0.918204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sachant que le conjoint est exonéré des frais ...</td>\n",
       "      <td>0.655484</td>\n",
       "      <td>39.257036</td>\n",
       "      <td>0.827742</td>\n",
       "      <td>0.989863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bonjour, Mon époux vient de rectifier son nom ...</td>\n",
       "      <td>0.575318</td>\n",
       "      <td>41.258762</td>\n",
       "      <td>0.787659</td>\n",
       "      <td>0.994167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bonjour puis je vous envoyer le papier par mai...</td>\n",
       "      <td>0.449137</td>\n",
       "      <td>24.248285</td>\n",
       "      <td>0.724568</td>\n",
       "      <td>0.951416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bonjour, Etant en pleine démission de mon CDI,...</td>\n",
       "      <td>0.636567</td>\n",
       "      <td>26.511257</td>\n",
       "      <td>0.818283</td>\n",
       "      <td>0.963961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Questions  score moyen_dense  \\\n",
       "0  quellel est la date limite pour demander une b...           0.480306   \n",
       "1  Sachant que le conjoint est exonéré des frais ...           0.655484   \n",
       "2  bonjour, Mon époux vient de rectifier son nom ...           0.575318   \n",
       "3  bonjour puis je vous envoyer le papier par mai...           0.449137   \n",
       "4  Bonjour, Etant en pleine démission de mon CDI,...           0.636567   \n",
       "\n",
       "   score moyen_sparse  proba moyenne_dense  proba moyen_sparse  \n",
       "0           19.480194             0.740153            0.918204  \n",
       "1           39.257036             0.827742            0.989863  \n",
       "2           41.258762             0.787659            0.994167  \n",
       "3           24.248285             0.724568            0.951416  \n",
       "4           26.511257             0.818283            0.963961  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#premiere colonne du csv: liste des questions fausses pour dense, bonnes pour sparse\n",
    "col1 = [dense_fails_sparse_good[k] for k in range(len(dense_fails_sparse_good))] \n",
    "\n",
    "#deuxième colonne csv: liste des scores correspondants pour dense\n",
    "from statistics import mean\n",
    "tmp = [dense_errors[dense_fails_sparse_good[k]]['pred_fiches'] for k in range(len(col1))]\n",
    "L=[]\n",
    "M =[]\n",
    "for j in range(len(tmp)):\n",
    "        l = [tmp[j][k][2] for k in range(len(tmp[j]))]\n",
    "        m = mean(l)\n",
    "        L.append(m)\n",
    "#troisième colonne: liste des scores correspondants pour sparse\n",
    "tmp = [sparse_successes[dense_fails_sparse_good[k]]['pred_fiches'] for k in range(len(col1))]\n",
    "for j in range(len(tmp)):\n",
    "    l = [tmp[j][k][2] for k in range(len(tmp[j]))]\n",
    "    m1 = mean(l)\n",
    "    M.append(m1)\n",
    "\n",
    "#colonnes des probas\n",
    "\n",
    "tmp = [dense_errors[dense_fails_sparse_good[k]]['pred_fiches'] for k in range(len(col1))]\n",
    "A=[]\n",
    "B =[]\n",
    "for j in range(len(tmp)):\n",
    "        l = [tmp[j][k][3] for k in range(len(tmp[j]))]\n",
    "        a = mean(l)\n",
    "        A.append(a)\n",
    "#troisième colonne: liste des scores correspondants pour sparse\n",
    "tmp = [sparse_successes[dense_fails_sparse_good[k]]['pred_fiches'] for k in range(len(col1))]\n",
    "for j in range(len(tmp)):\n",
    "    l = [tmp[j][k][3] for k in range(len(tmp[j]))]\n",
    "    b = mean(l)\n",
    "    B.append(b)\n",
    "    \n",
    "\n",
    "df = pd.DataFrame(list(zip(col1, L, M,A,B)), columns =['Questions', 'score moyen_dense','score moyen_sparse','proba moyenne_dense','proba moyen_sparse']) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('Erreurs du retriever dense et succes du retriever sparse.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "- remplacer la colonne 'score moyen_sparse' par le score exact qui correspond à la bonne fiche parmi les fiches prédites\n",
    "- ramener le score du sparse sur (0,1)\n",
    "- **au vu de l'analyse faite sur le score de BM25 (i.e. les scores des deux retrievers ont un sens assez différent), est-il vraiment pertinant de faire un mélange des deux / établir un lien entre les deux? Serait-il peut etre plus intéressant de considérer les deux indépendament et établir un seuil pour le score à partir du quel on affichera la réponse ou pas?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
