{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, on fait une analyse quantitative pour déterminer à quel niveau les deux retrievers sont complémentaries. On essaie ensuite de déterminer des seuils pour le score qui nous permetteraient de choisir un des deux/en faire un mélange pour incrementer la performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse complémentarité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_json = json.load(open(\"f615_detailed_results.json\"))\n",
    "sparse_json = json.load(open(\"9abf_detailed_results.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dense_errors = dense_json[\"errors\"]\n",
    "sparse_errors = sparse_json[\"errors\"]\n",
    "dense_successes = dense_json[\"successes\"]\n",
    "sparse_successes = sparse_json[\"successes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_bon = set(sparse_successes.keys())\n",
    "sparse_mauvais = set(sparse_errors.keys())\n",
    "dense_bon = set(dense_successes.keys())\n",
    "dense_mauvais = set(dense_errors.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erreurs communes au deux retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etant donné notre échantillon de réponses correctes et d'erreurs pour le retriever sparse et le retriever dense, vérifions tout d'abord si les deux se trompent souvent sur les memes questions et si oui, en quel pourcentage par rapport aux errors commis au total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_errors = list(dense_mauvais.intersection(sparse_mauvais))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "0.2664359861591695\n"
     ]
    }
   ],
   "source": [
    "print(len(common_errors))\n",
    "print(len(common_errors)/(len(dense_errors)+len(sparse_errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dans cet échantillon, 26% des erreurs commis sont commis par les deux retrivers. Il est peut etre intéressant d'aller plus loin dans l'analyse de ces erreurs qui semblent très difficiles à résoudre pour les deux retrievers.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation des cas où le retriever dense fait des erreurs et le sparse trouve la bonne réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "dense_fails_sparse_good = list(sparse_bon.intersection(dense_mauvais))\n",
    "print(len(dense_fails_sparse_good))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pour 95 questions, le retriever sparse donne des bonnes réponses là où le dense se trompe.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5523255813953488\n"
     ]
    }
   ],
   "source": [
    "print(len(dense_fails_sparse_good)/len(dense_mauvais))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sur les 172 erreurs faites par le retriever dense, dans 55% des cas le sparse peut donner la bonne réponse, et il pourrait donc etre utile de mélanger les deux.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation des cas où le retriever sparse fait des erreurs et le dense trouve la bonne réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "sparse_fails_dense_good = list(sparse_mauvais.intersection(dense_bon))\n",
    "print(len(sparse_fails_dense_good))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pour 40 questions, le retriever dense donne des bonnes réponses là où le sparse se trompe.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3418803418803419\n"
     ]
    }
   ],
   "source": [
    "print(len(sparse_fails_dense_good)/len(sparse_mauvais))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sur les 117 erreurs faites par le retriever sparse, dans 34% des cas le dense peut donner la bonne réponse.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "Le retriever sparse, si mélangé au retriever dense, peut aider à l'améliorer (le sparse donne globalement des meilleures réponses donc c'est assez normal). Dans 34% des cas, le retriever dense peut apporter des bonnes réponses là où le sparse se trompe. Globalement, on ne peut pas parler de complémentarité exacte (dans 26% des cas les deux se trompent et un mélange des deux n'apporteraient donc aucune amélioration), mais coupler le sparse au dense peut augmenter le nombre de bonnes réponses par rapport au dense seul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suite de l'analyse: peut-on trouver un seuil pour 'score' ou 'proba' qui nous indique qu'il faudrait utiliser le retriever dense pour améliorer le sparse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*exemple de ce qu'on souhaiterait avoir : if [score (dense) < 0.2 et score (sparse) > 0.7 ]: use sparse*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un dataframe qui contient la liste des questions où le retriever dense améliore le sparse, avec les scores \n",
    "correspondantes. En particulier:\n",
    "- colonne 1 *Questions* : questions où le sparse se trompe et le dense donne la bonne réponse\n",
    "- colonne 2 *k* : valeur (ou valeurs) de k correspondant à la bonne réponse donnée par le dense\n",
    "- colonne 3 *scores_dense*: scores donnés par le retriever dense pour la question et la valeur de k correspondante\n",
    "- colonne 4 *best_score_sparse*: meilleur score obtenu pour cette question par le retriever sparse, qui fait une erreur (*attention ici je dois corriger le code car pas de correspondance de lignes dans le csv pour cette colonne*)\n",
    "- colonne 5 *score_sparse_0_1*: normalisation sur ]0,1[ du score pour le retriever sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colonne des questions\n",
    "col1 = [sparse_fails_dense_good[k] for k in range(len(sparse_fails_dense_good))] \n",
    "\n",
    "#voici une liste qui nous donne les true fiches pour chaque success du sparse\n",
    "\n",
    "true_fiches_col1 = [dense_successes[sparse_fails_dense_good[k]]['true_fiches'] for k in range(len(col1))]\n",
    "fiches_pred_dense = [dense_successes[sparse_fails_dense_good[k]]['pred_fiches'] for k in range(len(col1))]\n",
    "\n",
    "def col_questions(col1,true_fiches_col1,fiches_pred_dense):\n",
    "    A=[]\n",
    "    for k in range(len(fiches_pred_dense)):\n",
    "        for j in range(len(fiches_pred_dense[k])):\n",
    "            if (str(true_fiches_col1[k][0]) in fiches_pred_dense[k][j][0]):\n",
    "                A.append(col1[k])\n",
    "                \n",
    "    return A\n",
    "\n",
    "def col_k_dense(true_fiches_col1,fiches_pred_dense):\n",
    "    M=[]\n",
    "    for k in range(len(fiches_pred_dense)):\n",
    "        for j in range(len(fiches_pred_dense[k])):\n",
    "            if (str(true_fiches_col1[k][0]) in fiches_pred_dense[k][j][0]):\n",
    "                M.append(fiches_pred_dense[k][j][1])\n",
    "    return M\n",
    "\n",
    "\n",
    "def col_scores_k_dense(true_fiches_col1,fiches_pred_dense):\n",
    "    L=[]\n",
    "    for k in range(len(fiches_pred_dense)):\n",
    "        for j in range(len(fiches_pred_dense[k])):\n",
    "            if (str(true_fiches_col1[k][0]) in fiches_pred_dense[k][j][0]):\n",
    "                L.append(fiches_pred_dense[k][j][2])\n",
    "                \n",
    "    return L\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def col_k_dense(true_fiches_col1,fiches_pred_dense):\n",
    "    M=[]\n",
    "    for k in range(len(fiches_pred_dense)):\n",
    "        for j in range(len(fiches_pred_dense[k])):\n",
    "            if (str(true_fiches_col1[k][0]) in fiches_pred_dense[k][j][0]):\n",
    "                M.append(fiches_pred_dense[k][j][1])\n",
    "    return M\n",
    "\n",
    "\n",
    "fiches_pred_sparse = [sparse_errors[sparse_fails_dense_good[k]]['pred_fiches'] for k in range(len(col1))]\n",
    "\n",
    "def col_best_score_sparse(fiches_pred_sparse):\n",
    "    J =[]\n",
    "    for j in range(len(fiches_pred_sparse)):\n",
    "        l = [fiches_pred_sparse[j][k][2] for k in range(len(fiches_pred_sparse[j]))]\n",
    "        J.append(max(l))\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = col_questions(col1,true_fiches_col1,fiches_pred_dense)\n",
    "col2 = col_k_dense(true_fiches_col1,fiches_pred_dense)\n",
    "col3 = col_scores_k_dense(true_fiches_col1,fiches_pred_dense)\n",
    "col4 = col_best_score_sparse(fiches_pred_sparse) \n",
    "col4bis = [(col4[x]-min(col4))/(max(col4)-min(col4))for x in range(len(col4))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>k</th>\n",
       "      <th>scores_dense</th>\n",
       "      <th>best_score_sparse</th>\n",
       "      <th>score_sparse_0_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pour Un enfant de 8 ans reconnu par ses parent...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.576475</td>\n",
       "      <td>39.888540</td>\n",
       "      <td>0.600575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pour Un enfant de 8 ans reconnu par ses parent...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.572860</td>\n",
       "      <td>38.415104</td>\n",
       "      <td>0.575890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pour Un enfant de 8 ans reconnu par ses parent...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.560518</td>\n",
       "      <td>25.564970</td>\n",
       "      <td>0.360610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pour Un enfant de 8 ans reconnu par ses parent...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.560518</td>\n",
       "      <td>60.007965</td>\n",
       "      <td>0.937638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pour Un enfant de 8 ans reconnu par ses parent...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.560518</td>\n",
       "      <td>14.889669</td>\n",
       "      <td>0.181766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bonjour, lorsque j'ai fait faire la carte d'id...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.664292</td>\n",
       "      <td>9.055583</td>\n",
       "      <td>0.084026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>En tant que mineur, ai-je le droit d'avoir des...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.655366</td>\n",
       "      <td>27.497390</td>\n",
       "      <td>0.392984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>En tant que mineur, ai-je le droit d'avoir des...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.632048</td>\n",
       "      <td>22.873125</td>\n",
       "      <td>0.315513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>En tant que mineur, ai-je le droit d'avoir des...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.631823</td>\n",
       "      <td>28.692795</td>\n",
       "      <td>0.413011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bonjour, Mon conjoint et moi meme nous sommes ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.563578</td>\n",
       "      <td>38.956085</td>\n",
       "      <td>0.584953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Questions  k  scores_dense  \\\n",
       "0  Pour Un enfant de 8 ans reconnu par ses parent...  1      0.576475   \n",
       "1  Pour Un enfant de 8 ans reconnu par ses parent...  2      0.572860   \n",
       "2  Pour Un enfant de 8 ans reconnu par ses parent...  3      0.560518   \n",
       "3  Pour Un enfant de 8 ans reconnu par ses parent...  4      0.560518   \n",
       "4  Pour Un enfant de 8 ans reconnu par ses parent...  5      0.560518   \n",
       "5  Bonjour, lorsque j'ai fait faire la carte d'id...  4      0.664292   \n",
       "6  En tant que mineur, ai-je le droit d'avoir des...  1      0.655366   \n",
       "7  En tant que mineur, ai-je le droit d'avoir des...  2      0.632048   \n",
       "8  En tant que mineur, ai-je le droit d'avoir des...  3      0.631823   \n",
       "9  Bonjour, Mon conjoint et moi meme nous sommes ...  1      0.563578   \n",
       "\n",
       "   best_score_sparse  score_sparse_0_1  \n",
       "0          39.888540          0.600575  \n",
       "1          38.415104          0.575890  \n",
       "2          25.564970          0.360610  \n",
       "3          60.007965          0.937638  \n",
       "4          14.889669          0.181766  \n",
       "5           9.055583          0.084026  \n",
       "6          27.497390          0.392984  \n",
       "7          22.873125          0.315513  \n",
       "8          28.692795          0.413011  \n",
       "9          38.956085          0.584953  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list(zip(col,col2,col3,col4,col4bis)), columns =['Questions','k', 'scores_dense','best_score_sparse','score_sparse_0_1'])\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pb: pas la bonne correspondance entre les questions est le best score pour le retriever sparse, à résoudre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('erreurs_retriever_sparse_succes_retriever_dense.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "- résoudre problème au niveau du csv pour le best_score_sparse\n",
    "- **est-il pertinant de trouver un seuil du score qui relie les deux retrievers? Si oui, comment**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
